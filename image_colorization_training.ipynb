{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6hO0rt_SbA"
      },
      "source": [
        "# ðŸŽ¨ Image Colorization Training - Complete Notebook\n",
        "\n",
        "**Complete end-to-end image colorization training with U-Net architecture**\n",
        "\n",
        "- âœ… **50 epochs training**\n",
        "- âœ… **Checkpoint saving every epoch**\n",
        "- âœ… **Error calculation every epoch**\n",
        "- âœ… **TPU/GPU/CPU support**\n",
        "- âœ… **Memory optimized for Kaggle**\n",
        "- âœ… **COCO dataset compatible**\n",
        "\n",
        "**Hardware Support:**\n",
        "- ðŸš€ TPU (Google Colab/Kaggle TPU)\n",
        "- ðŸŽ® GPU (CUDA)\n",
        "- ðŸ’» CPU (fallback)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ffeu_i_SbB"
      },
      "source": [
        "## ðŸ“¦ Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nieaWVXE_SbD",
        "outputId": "b857518a-f769-41ff-d01b-5550b8b16741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… TPU support installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch>=2.0.0 torchvision>=0.15.0\n",
        "!pip install opencv-python>=4.8.0 Pillow>=10.0.0 scikit-image>=0.21.0\n",
        "!pip install numpy>=1.24.0 scipy>=1.11.0 matplotlib>=3.7.0\n",
        "!pip install tqdm>=4.65.0 pyyaml>=6.0 psutil>=5.9.0\n",
        "\n",
        "# TPU support (for Kaggle TPU)\n",
        "try:\n",
        "    !pip install torch-xla>=2.0.0\n",
        "    print(\"âœ… TPU support installed\")\n",
        "except:\n",
        "    print(\"âš ï¸ TPU support not available (GPU/CPU mode)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyvKr1Lg_SbD"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional, Dict, List\n",
        "import json\n",
        "import psutil\n",
        "from skimage import color\n",
        "\n",
        "# TPU support\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "    TPU_AVAILABLE = True\n",
        "    print(\"âœ… TPU support loaded\")\n",
        "except ImportError:\n",
        "    TPU_AVAILABLE = False\n",
        "    print(\"âš ï¸ TPU not available\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if TPU_AVAILABLE:\n",
        "    print(f\"TPU available: {xm.xla_device() is not None}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCPIsAPt_SbE"
      },
      "source": [
        "## ðŸ—ï¸ Model Architecture - U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlR9Zohl_SbE"
      },
      "outputs": [],
      "source": [
        "class UNetColorizer(nn.Module):\n",
        "    \"\"\"U-Net architecture for image colorization.\"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=1, output_channels=2):\n",
        "        super(UNetColorizer, self).__init__()\n",
        "\n",
        "        # Encoder (downsampling)\n",
        "        self.enc1 = self._conv_block(input_channels, 64)\n",
        "        self.enc2 = self._conv_block(64, 128)\n",
        "        self.enc3 = self._conv_block(128, 256)\n",
        "        self.enc4 = self._conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._conv_block(512, 1024)\n",
        "\n",
        "        # Decoder (upsampling)\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = self._conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = self._conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = self._conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = self._conv_block(128, 64)\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(64, output_channels, 1)\n",
        "\n",
        "        # Pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def _conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat([dec4, enc4], dim=1)\n",
        "        dec4 = self.dec4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
        "        dec3 = self.dec3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
        "        dec2 = self.dec2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
        "        dec1 = self.dec1(dec1)\n",
        "\n",
        "        return torch.tanh(self.final(dec1))\n",
        "\n",
        "print(\"âœ… U-Net model defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ufRosl7_SbE"
      },
      "source": [
        "## ðŸŽ¯ Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0znZEWA0_SbE"
      },
      "outputs": [],
      "source": [
        "class L1Loss(nn.Module):\n",
        "    \"\"\"L1 loss for colorization training.\"\"\"\n",
        "\n",
        "    def __init__(self, weight: float = 1.0):\n",
        "        super(L1Loss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, predicted: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        return self.weight * F.l1_loss(predicted, target)\n",
        "\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    \"\"\"VGG-based perceptual loss.\"\"\"\n",
        "\n",
        "    def __init__(self, weight: float = 1.0):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "        # Load pre-trained VGG16\n",
        "        vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg.features)[:16])\n",
        "\n",
        "        # Freeze parameters\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def lab_to_rgb_tensor(self, lab_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert LAB tensor to RGB tensor.\"\"\"\n",
        "        # Denormalize LAB values\n",
        "        lab = lab_tensor.clone()\n",
        "        lab[:, 0] = lab[:, 0] * 100  # L: 0-100\n",
        "        lab[:, 1] = lab[:, 1] * 127  # a: -127 to 127\n",
        "        lab[:, 2] = lab[:, 2] * 127  # b: -127 to 127\n",
        "\n",
        "        # Convert to RGB (simplified conversion)\n",
        "        rgb = torch.zeros_like(lab)\n",
        "        rgb[:, 0] = lab[:, 0] / 100  # Approximate conversion\n",
        "        rgb[:, 1] = (lab[:, 1] + 127) / 255\n",
        "        rgb[:, 2] = (lab[:, 2] + 127) / 255\n",
        "\n",
        "        return torch.clamp(rgb, 0, 1)\n",
        "\n",
        "    def forward(self, predicted_lab: torch.Tensor, target_lab: torch.Tensor) -> torch.Tensor:\n",
        "        # Convert LAB to RGB\n",
        "        predicted_rgb = self.lab_to_rgb_tensor(predicted_lab)\n",
        "        target_rgb = self.lab_to_rgb_tensor(target_lab)\n",
        "\n",
        "        # Extract features\n",
        "        predicted_features = self.feature_extractor(predicted_rgb)\n",
        "        target_features = self.feature_extractor(target_rgb)\n",
        "\n",
        "        return self.weight * F.mse_loss(predicted_features, target_features)\n",
        "\n",
        "\n",
        "class HybridLoss(nn.Module):\n",
        "    \"\"\"Hybrid loss combining L1 and perceptual losses.\"\"\"\n",
        "\n",
        "    def __init__(self, l1_weight: float = 1.0, perceptual_weight: float = 0.1):\n",
        "        super(HybridLoss, self).__init__()\n",
        "        self.l1_loss = L1Loss(l1_weight)\n",
        "        self.perceptual_loss = PerceptualLoss(perceptual_weight) if perceptual_weight > 0 else None\n",
        "\n",
        "    def forward(self, predicted_ab, target_ab, predicted_lab, target_lab):\n",
        "        # L1 loss on ab channels\n",
        "        l1_loss_value = self.l1_loss(predicted_ab, target_ab)\n",
        "\n",
        "        # Perceptual loss on full LAB images\n",
        "        if self.perceptual_loss is not None:\n",
        "            perceptual_loss_value = self.perceptual_loss(predicted_lab, target_lab)\n",
        "        else:\n",
        "            perceptual_loss_value = torch.tensor(0.0, device=predicted_ab.device)\n",
        "\n",
        "        total_loss = l1_loss_value + perceptual_loss_value\n",
        "\n",
        "        return total_loss, l1_loss_value, perceptual_loss_value\n",
        "\n",
        "print(\"âœ… Loss functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iowvxpJG_SbF"
      },
      "source": [
        "## ðŸ“Š Dataset & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_BUXcP9_SbF"
      },
      "outputs": [],
      "source": [
        "class ColorizationDataset(Dataset):\n",
        "    \"\"\"Dataset for image colorization training.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: str, target_size: Tuple[int, int] = (256, 256)):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Load image paths\n",
        "        self.image_paths = self._load_image_paths()\n",
        "        print(f\"Found {len(self.image_paths)} images\")\n",
        "\n",
        "    def _load_image_paths(self) -> List[Path]:\n",
        "        \"\"\"Load all image paths from dataset directory.\"\"\"\n",
        "        extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
        "        image_paths = []\n",
        "\n",
        "        for ext in extensions:\n",
        "            image_paths.extend(self.dataset_path.glob(f'*{ext}'))\n",
        "            image_paths.extend(self.dataset_path.glob(f'*{ext.upper()}'))\n",
        "\n",
        "        return sorted(image_paths)\n",
        "\n",
        "    def _rgb_to_lab(self, rgb_image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Convert RGB image to LAB color space.\"\"\"\n",
        "        return color.rgb2lab(rgb_image)\n",
        "\n",
        "    def _load_and_preprocess_image(self, image_path: Path) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Load and preprocess a single image.\"\"\"\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize(self.target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        rgb_image = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "        # Convert to LAB color space\n",
        "        lab_image = self._rgb_to_lab(rgb_image)\n",
        "\n",
        "        # Normalize LAB values\n",
        "        lab_image[:, :, 0] = lab_image[:, :, 0] / 100.0  # L: 0-100 -> 0-1\n",
        "        lab_image[:, :, 1] = lab_image[:, :, 1] / 127.0  # a: -127-127 -> -1-1\n",
        "        lab_image[:, :, 2] = lab_image[:, :, 2] / 127.0  # b: -127-127 -> -1-1\n",
        "\n",
        "        # Extract L and ab channels\n",
        "        l_channel = lab_image[:, :, 0:1]  # L channel\n",
        "        ab_channels = lab_image[:, :, 1:3]  # ab channels\n",
        "\n",
        "        # Convert to tensors and rearrange dimensions to (C, H, W)\n",
        "        l_tensor = torch.from_numpy(l_channel).permute(2, 0, 1).float()\n",
        "        ab_tensor = torch.from_numpy(ab_channels).permute(2, 0, 1).float()\n",
        "        lab_tensor = torch.from_numpy(lab_image).permute(2, 0, 1).float()\n",
        "\n",
        "        return {\n",
        "            'l_channel': l_tensor,\n",
        "            'ab_channels': ab_tensor,\n",
        "            'lab_image': lab_tensor\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        image_path = self.image_paths[idx]\n",
        "        try:\n",
        "            return self._load_and_preprocess_image(image_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {image_path}: {e}\")\n",
        "            # Return a dummy sample\n",
        "            return {\n",
        "                'l_channel': torch.zeros(1, *self.target_size),\n",
        "                'ab_channels': torch.zeros(2, *self.target_size),\n",
        "                'lab_image': torch.zeros(3, *self.target_size)\n",
        "            }\n",
        "\n",
        "print(\"âœ… Dataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yxiGOBC_SbF"
      },
      "source": [
        "## ðŸš€ Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg48Jauu_SbF"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "CONFIG = {\n",
        "    # Dataset\n",
        "    'dataset_path': '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/train2014',  # Update this path\n",
        "    'input_size': [256, 256],\n",
        "\n",
        "    # Training\n",
        "    'epochs': 50,\n",
        "    'learning_rate': 0.0001,\n",
        "    'weight_decay': 0.0001,\n",
        "\n",
        "    # Loss function\n",
        "    'l1_weight': 1.0,\n",
        "    'perceptual_weight': 0.0,  # Disabled to save memory\n",
        "\n",
        "    # Device-specific batch sizes (will be set automatically)\n",
        "    'batch_size_tpu': 16,\n",
        "    'batch_size_gpu': 4,\n",
        "    'batch_size_cpu': 2,\n",
        "}\n",
        "\n",
        "print(\"âœ… Configuration loaded\")\n",
        "print(f\"Dataset path: {CONFIG['dataset_path']}\")\n",
        "print(f\"Training for {CONFIG['epochs']} epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4gczBld_SbF"
      },
      "source": [
        "## ðŸŽ¯ Complete Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoExKln0_SbF"
      },
      "outputs": [],
      "source": [
        "class ImageColorizationTrainer:\n",
        "    \"\"\"Complete image colorization trainer with multi-device support.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Setup device (TPU > CUDA > CPU)\n",
        "        self._setup_device()\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "        os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "        # Setup model\n",
        "        self.model = UNetColorizer(input_channels=1, output_channels=2).to(self.device)\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=config['learning_rate'],\n",
        "            weight_decay=config['weight_decay']\n",
        "        )\n",
        "\n",
        "        # Setup loss\n",
        "        self.criterion = HybridLoss(\n",
        "            l1_weight=config['l1_weight'],\n",
        "            perceptual_weight=config['perceptual_weight']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Setup data\n",
        "        self._setup_data()\n",
        "\n",
        "        # Training metrics\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def _setup_device(self):\n",
        "        \"\"\"Setup device with priority: TPU > CUDA > CPU\"\"\"\n",
        "        if TPU_AVAILABLE and xm.xla_device() is not None:\n",
        "            self.device = xm.xla_device()\n",
        "            self.is_tpu = True\n",
        "            self.is_cuda = False\n",
        "            print(f\"ðŸš€ Using TPU device: {self.device}\")\n",
        "            print(f\"TPU cores: {xm.xrt_world_size()}\")\n",
        "        elif torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "            self.is_tpu = False\n",
        "            self.is_cuda = True\n",
        "            print(f\"ðŸŽ® Using CUDA device: {self.device}\")\n",
        "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            self.is_tpu = False\n",
        "            self.is_cuda = False\n",
        "            print(f\"ðŸ’» Using CPU device: {self.device}\")\n",
        "\n",
        "    def _setup_data(self):\n",
        "        \"\"\"Setup training and validation data loaders.\"\"\"\n",
        "        # Load dataset\n",
        "        dataset = ColorizationDataset(\n",
        "            dataset_path=self.config['dataset_path'],\n",
        "            target_size=tuple(self.config['input_size'])\n",
        "        )\n",
        "\n",
        "        # Split dataset\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print(f\"ðŸ“Š Dataset: {len(dataset):,} images\")\n",
        "        print(f\"Training: {len(train_dataset):,} images\")\n",
        "        print(f\"Validation: {len(val_dataset):,} images\")\n",
        "\n",
        "        # Device-specific batch sizes and settings\n",
        "        if self.is_tpu:\n",
        "            batch_size = self.config['batch_size_tpu']\n",
        "            num_workers = 0\n",
        "            pin_memory = False\n",
        "        elif self.is_cuda:\n",
        "            batch_size = self.config['batch_size_gpu']\n",
        "            num_workers = 2\n",
        "            pin_memory = True\n",
        "        else:\n",
        "            batch_size = self.config['batch_size_cpu']\n",
        "            num_workers = 4\n",
        "            pin_memory = False\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "        # Wrap with TPU parallel loader if using TPU\n",
        "        if self.is_tpu:\n",
        "            self.train_loader = pl.ParallelLoader(train_loader, [self.device])\n",
        "            self.val_loader = pl.ParallelLoader(val_loader, [self.device])\n",
        "        else:\n",
        "            self.train_loader = train_loader\n",
        "            self.val_loader = val_loader\n",
        "\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Train batches: {len(train_loader):,}\")\n",
        "        print(f\"Val batches: {len(val_loader):,}\")\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Get the actual loader for TPU or regular loader\n",
        "        loader = self.train_loader.per_device_loader(self.device) if self.is_tpu else self.train_loader\n",
        "\n",
        "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']} - Training\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            if self.is_tpu:\n",
        "                grayscale = batch['l_channel'].to(self.device)\n",
        "                color_ab = batch['ab_channels'].to(self.device)\n",
        "            else:\n",
        "                grayscale = batch['l_channel'].to(self.device, non_blocking=True)\n",
        "                color_ab = batch['ab_channels'].to(self.device, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            predicted_ab = self.model(grayscale)\n",
        "\n",
        "            # Construct full LAB images for loss calculation\n",
        "            predicted_lab = torch.cat([grayscale, predicted_ab], dim=1)\n",
        "            target_lab = torch.cat([grayscale, color_ab], dim=1)\n",
        "\n",
        "            total_loss_batch, l1_loss, perceptual_loss = self.criterion(\n",
        "                predicted_ab, color_ab, predicted_lab, target_lab\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss_batch.backward()\n",
        "\n",
        "            if self.is_tpu:\n",
        "                xm.optimizer_step(self.optimizer)\n",
        "            else:\n",
        "                self.optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_batch.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{total_loss_batch.item():.4f}',\n",
        "                'Avg': f'{total_loss/(batch_idx+1):.4f}'\n",
        "            })\n",
        "\n",
        "            # Clear cache periodically (only for CUDA)\n",
        "            if self.is_cuda and (batch_idx + 1) % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def validate_epoch(self, epoch):\n",
        "        \"\"\"Validate for one epoch.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Get the actual loader for TPU or regular loader\n",
        "        loader = self.val_loader.per_device_loader(self.device) if self.is_tpu else self.val_loader\n",
        "\n",
        "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.config['epochs']} - Validation\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(progress_bar):\n",
        "                if self.is_tpu:\n",
        "                    grayscale = batch['l_channel'].to(self.device)\n",
        "                    color_ab = batch['ab_channels'].to(self.device)\n",
        "                else:\n",
        "                    grayscale = batch['l_channel'].to(self.device, non_blocking=True)\n",
        "                    color_ab = batch['ab_channels'].to(self.device, non_blocking=True)\n",
        "\n",
        "                predicted_ab = self.model(grayscale)\n",
        "\n",
        "                # Construct full LAB images for loss calculation\n",
        "                predicted_lab = torch.cat([grayscale, predicted_ab], dim=1)\n",
        "                target_lab = torch.cat([grayscale, color_ab], dim=1)\n",
        "\n",
        "                total_loss_batch, l1_loss, perceptual_loss = self.criterion(\n",
        "                    predicted_ab, color_ab, predicted_lab, target_lab\n",
        "                )\n",
        "\n",
        "                total_loss += total_loss_batch.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\n",
        "                    'Loss': f'{total_loss_batch.item():.4f}',\n",
        "                    'Avg': f'{total_loss/(batch_idx+1):.4f}'\n",
        "                })\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def save_checkpoint(self, epoch, train_loss, val_loss):\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'config': self.config\n",
        "        }\n",
        "\n",
        "        # Save epoch checkpoint\n",
        "        checkpoint_path = f\"checkpoints/epoch_{epoch+1:02d}.pth\"\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        # Save best model\n",
        "        if not hasattr(self, 'best_val_loss') or val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            torch.save(checkpoint, \"checkpoints/best_model.pth\")\n",
        "            print(f\"âœ… New best model saved (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "        print(f\"ðŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    def log_metrics(self, epoch, train_loss, val_loss):\n",
        "        \"\"\"Log training metrics.\"\"\"\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.val_losses.append(val_loss)\n",
        "\n",
        "        # Write to log file\n",
        "        log_entry = f\"{datetime.now().isoformat()},Epoch,{epoch+1},Train,{train_loss:.6f},Val,{val_loss:.6f}\\n\"\n",
        "        with open(\"logs/training_log.csv\", \"a\") as f:\n",
        "            f.write(log_entry)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"ðŸ“Š Epoch {epoch+1:2d}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
        "\n",
        "        if hasattr(self, 'best_val_loss'):\n",
        "            print(f\"    Best Val Loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop.\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"ðŸš€ Starting Image Colorization Training\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Initialize log file\n",
        "        with open(\"logs/training_log.csv\", \"w\") as f:\n",
        "            f.write(\"timestamp,type,epoch,metric,train_loss,val_metric,val_loss\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config['epochs']):\n",
        "            print(f\"\\n{'='*20} EPOCH {epoch+1}/{self.config['epochs']} {'='*20}\")\n",
        "\n",
        "            # Train\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "\n",
        "            # Validate\n",
        "            val_loss = self.validate_epoch(epoch)\n",
        "\n",
        "            # Save checkpoint\n",
        "            self.save_checkpoint(epoch, train_loss, val_loss)\n",
        "\n",
        "            # Log metrics\n",
        "            self.log_metrics(epoch, train_loss, val_loss)\n",
        "\n",
        "            # Memory cleanup\n",
        "            if self.is_cuda:\n",
        "                torch.cuda.empty_cache()\n",
        "            elif self.is_tpu:\n",
        "                xm.mark_step()\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nðŸŽ‰ Training completed in {total_time/3600:.1f} hours!\")\n",
        "        print(f\"ðŸ“ Checkpoints saved in: checkpoints/\")\n",
        "        print(f\"ðŸ“Š Training log saved in: logs/training_log.csv\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "print(\"âœ… Training class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOKQkXhV_SbG"
      },
      "source": [
        "## ðŸŽ¬ Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3mECR1m_SbG"
      },
      "outputs": [],
      "source": [
        "# Update dataset path for your Kaggle dataset\n",
        "# Common Kaggle COCO paths:\n",
        "# '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/train2014'\n",
        "# '/kaggle/input/coco2014/train2014'\n",
        "# '/kaggle/input/ms-coco-2014/train2014'\n",
        "\n",
        "# Check available datasets\n",
        "import os\n",
        "print(\"Available input datasets:\")\n",
        "if os.path.exists('/kaggle/input'):\n",
        "    for item in os.listdir('/kaggle/input'):\n",
        "        print(f\"  - {item}\")\n",
        "        if 'coco' in item.lower():\n",
        "            dataset_path = f'/kaggle/input/{item}'\n",
        "            print(f\"    Contents: {os.listdir(dataset_path)}\")\n",
        "else:\n",
        "    print(\"Not running on Kaggle - update CONFIG['dataset_path'] manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C29IASmP_SbG"
      },
      "outputs": [],
      "source": [
        "# Update the dataset path based on your Kaggle dataset\n",
        "# CONFIG['dataset_path'] = '/kaggle/input/your-coco-dataset/train2014'  # Update this!\n",
        "\n",
        "# Create trainer and start training\n",
        "trainer = ImageColorizationTrainer(CONFIG)\n",
        "\n",
        "# Start training for 50 epochs\n",
        "train_losses, val_losses = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHzLN2Ko_SbG"
      },
      "source": [
        "## ðŸ“ˆ Training Results & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Lj4e4r_SbG"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\nðŸŽ¯ Final Results:\")\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"Best Validation Loss: {min(val_losses):.4f} (Epoch {val_losses.index(min(val_losses))+1})\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'best_val_loss': min(val_losses),\n",
        "    'best_epoch': val_losses.index(min(val_losses)) + 1,\n",
        "    'config': CONFIG\n",
        "}\n",
        "\n",
        "with open('training_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Results saved to training_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjK1E1__SbG"
      },
      "source": [
        "## ðŸŽ¨ Test the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXkiNoGb_SbG"
      },
      "outputs": [],
      "source": [
        "def colorize_image(model, image_path, device):\n",
        "    \"\"\"Colorize a single image using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "\n",
        "    # Convert to LAB\n",
        "    rgb_image = np.array(image).astype(np.float32) / 255.0\n",
        "    lab_image = color.rgb2lab(rgb_image)\n",
        "\n",
        "    # Normalize and extract L channel\n",
        "    l_channel = lab_image[:, :, 0:1] / 100.0\n",
        "    l_tensor = torch.from_numpy(l_channel).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "\n",
        "    # Predict ab channels\n",
        "    with torch.no_grad():\n",
        "        predicted_ab = model(l_tensor)\n",
        "\n",
        "    # Convert back to numpy\n",
        "    predicted_ab = predicted_ab.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Reconstruct LAB image\n",
        "    reconstructed_lab = np.zeros_like(lab_image)\n",
        "    reconstructed_lab[:, :, 0] = l_channel.squeeze() * 100.0\n",
        "    reconstructed_lab[:, :, 1:] = predicted_ab * 127.0\n",
        "\n",
        "    # Convert back to RGB\n",
        "    reconstructed_rgb = color.lab2rgb(reconstructed_lab)\n",
        "    reconstructed_rgb = np.clip(reconstructed_rgb, 0, 1)\n",
        "\n",
        "    return rgb_image, reconstructed_rgb\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = \"checkpoints/best_model.pth\"\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path, map_location=trainer.device)\n",
        "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"âœ… Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "    # Test on a few images from the dataset\n",
        "    test_images = list(Path(CONFIG['dataset_path']).glob('*.jpg'))[:5]\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, img_path in enumerate(test_images):\n",
        "        try:\n",
        "            original, colorized = colorize_image(trainer.model, img_path, trainer.device)\n",
        "\n",
        "            plt.subplot(2, 5, i+1)\n",
        "            plt.imshow(original)\n",
        "            plt.title(f'Original {i+1}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(2, 5, i+6)\n",
        "            plt.imshow(colorized)\n",
        "            plt.title(f'Colorized {i+1}')\n",
        "            plt.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('colorization_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âŒ No trained model found. Train the model first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzdkQmaa_SbH"
      },
      "source": [
        "## ðŸ“ Download Results (for Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ7ojgET_SbH"
      },
      "outputs": [],
      "source": [
        "# Create a zip file with all results for easy download\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('colorization_training_results.zip', 'w') as zipf:\n",
        "    # Add checkpoints\n",
        "    for checkpoint_file in Path('checkpoints').glob('*.pth'):\n",
        "        zipf.write(checkpoint_file, f'checkpoints/{checkpoint_file.name}')\n",
        "\n",
        "    # Add logs\n",
        "    if os.path.exists('logs/training_log.csv'):\n",
        "        zipf.write('logs/training_log.csv', 'logs/training_log.csv')\n",
        "\n",
        "    # Add results\n",
        "    if os.path.exists('training_results.json'):\n",
        "        zipf.write('training_results.json', 'training_results.json')\n",
        "\n",
        "    # Add plots\n",
        "    if os.path.exists('training_curves.png'):\n",
        "        zipf.write('training_curves.png', 'training_curves.png')\n",
        "\n",
        "    if os.path.exists('colorization_results.png'):\n",
        "        zipf.write('colorization_results.png', 'colorization_results.png')\n",
        "\n",
        "print(\"ðŸ“¦ All results packaged in: colorization_training_results.zip\")\n",
        "print(\"\\nðŸŽ‰ Training Complete!\")\n",
        "print(\"\\nðŸ“‹ Summary:\")\n",
        "print(f\"   - Trained for {CONFIG['epochs']} epochs\")\n",
        "print(f\"   - Model checkpoints saved every epoch\")\n",
        "print(f\"   - Training and validation losses calculated\")\n",
        "print(f\"   - Best model saved automatically\")\n",
        "print(f\"   - Results visualized and saved\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}